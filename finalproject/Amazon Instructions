I. Create S3 bucket using the following instructions:
	(http://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)



II. How to login to EC2 compute node

	(https://console.aws.amazon.com/ec2/home?region=us-east-1#s=Instances)


	ssh -i keypair.pem ec2_name
	e.g.
	ssh -i ~/.ssh/kp1.pem  ec2-user@ec2-54-221-46-168.compute-1.amazonaws.com


	1) Install packages
	sudo yum install gcc-c++
	wget http://09c8d0b2229f813c1b93-c95ac804525aac4b6dba79b00b39d1d3.r79.cf1.rackcdn.com/Anaconda-1.6.1-Linux-x86_64.sh
	
	sudo apt-get install libgeos++-dev
	sudo pip install shapely
	sudo pip install pyshp

	wget http://download.osgeo.org/libspatialindex/spatialindex-src-1.7.1.tar.gz ; configure; make; make test; make install

	sudo pip install rtree # v 0.7.0 depends on libspatial 1.7.1 (getting lots of libspatial problems****)

	wget http://sourceforge.net/projects/s3tools/files/s3cmd/1.5.0-alpha1/s3cmd-1.5.0-alpha1.tar.gz
	tar zxvf s3cmd-1.5*
	sudo python setup.py install

	wget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-2.4.5.tgz


IIIA. Copy files between EC2 instance and S3 bucket

	# s3cmd put twitter.dump s3://mauriello/twitter/twitter.dump


IIIB. How to copy file from local machine to EC2 instance

    $ scp -i ~/.ssh/kp1.pem tw_stream.py ec2-user@ec2-54-242-16-10.compute-1.amazonaws.com:scripts/tw_stream.py

IIIC. 

How to get Access Key/Secret Key
https://console.aws.amazon.com/iam/home?#security_credential

Access Key ID:     AKIAIIKN443MA6DLSBKQ
Secret Access Key: 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

export AWS_ACCESS_KEY_ID=AKIAIIKN443MA6DLSBKQ
export AWS_SECRET_ACCESS_KEY=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



III. Startup Hadoop Cluster in the Elastic Map Reduce Console
	https://console.aws.amazon.com/elasticmapreduce/home?region=us-east-1#s=Home

	ssh -i ~/.ssh/kp1.pem hadoop@ec2-75-101-246-191.compute-1.amazonaws.com


